# 深度学习 Week_1



## 导语

为方便将来理解机器学习和深度学习，本周我主要了解相关基础概念：函数、神经网络、训练方法、矩阵、CNN、RNN，以及Transformer。另外，还尝试训练螺旋分类作业，对深度学习产生了初步体会和感知。信息来源：博客、B站“飞天闪客”博主、维基百科、ChatGPT。



## 符号主义与联结主义

**符号主义**试图通过完美拟合所有数据，寻找能够准确契合大量样本的函数。

**联结主义**通过“猜”问题，简化函数，找到足够接近真实答案的近似解。



## 神经网络

### 激活函数

在计算网络中， 一个节点的**激活函数**定义了该节点在给定的输入或输入的集合下的输出。简而言之：激活函数通过不断嵌套，将简单的线性关系转化为复杂的非线性关系，从而实现对训练数据的拟合。

例：

$f(x)=g(w_1x_1+b_1)$

$x_1=w_2x_2+b_2$

$x_2=w_3x_3+b_3$

…

其中，x_1可看作**隐藏层**。

### 人工神经网络

**人工神经网络**（英语：artificial neural network，ANNs）又称**类神经网络**，简称**神经网络**（neural network，NNs），在机器学习和认知科学领域，是一种模仿生物神经网络（动物的中枢神经系统，特别是大脑）的结构和功能的数学模型或计算模型，用于对函数进行估计或近似。



![undefined](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3d/Neural_network.svg/1280px-Neural_network.svg.png)



### 参数

**均方误差**（MSE)：**损失函数**的一种，表示所有样本函数值与实际值差方的平均值。目的：不断调整参数，求解让均方误差最小的 $w$ 和 $b$ 值，即实现**线性回归**。

**梯度下降**：让 $w$ 和 $b$ 在损失函数对其的偏导数的反方向进行变化，从而更快求出**局部极小值**，让损失函数减小。实现方法：通过链式法则，计算偏导数，让数值从右向左逐层变化，实现**反向传播**。



## 过拟合的解决方式

### 从数据和模型入手

简化模型复杂度，或增加数据量。在图像处理中，可通过对图像旋转、翻转、加噪声等方式实现**数据增强**，增强模型的**鲁棒性**。

### 从训练过程入手

提前终止训练过程；

**正则化**：新损失函数 = 损失函数 + 正则化系数 * 参数绝对值和或平方和

**Dropout**：在训练过程随机丢弃部分数据，减少在某些关键参数上的过度依赖。



## 训练时遇到的其他问题

**梯度消失**：网络越深，梯度在反向传播时越来越小，导致参数更新困难。

**梯度爆炸**：梯度数值越来越大，参数调整幅度失控。

…



## 矩阵

矩阵（Matrix）是深度学习中最基础的数学工具，几乎所有的模型结构和计算最终都可以表示为矩阵运算。

### 1. 表示数据与参数

输入数据通常由矩阵或张量表示（如图像的像素矩阵，文本的 embedding 矩阵）;

神经网络的权重参数通常以矩阵形式存储，例如全连接层的权重 WWW。

### 2. 执行线性变换

神经网络的前向传播例如

$y=Wx+by = Wx + by=Wx+b$

本质是矩阵乘法，加偏置，再执行激活函数。

### 3. 反向传播中的梯度计算

矩阵结构便于自动求导和批量计算

### 4. 批量处理（Batch Processing）

多个样本组成矩阵

$X=[x1,x2,...,xB]X = [x_1, x_2, ..., x_B]X=[x1,x2,...,xB]$

矩阵乘法能一次计算整个批次，提高 GPU 并行效率。

------

## CNN

**卷积神经网络**（CNN）是一类专门处理具有空间结构的数据（如图像和音频）的神经网络。

### 1. 核心特性

**局部感受野**：卷积核仅关注局部区域，捕捉边缘、纹理等特征。

**权重共享**：同一卷积核在整张图上滑动，显著减少参数量。

**平移不变性**：图像内容移动不会影响识别效果。

### 2. 主要组成结构

**卷积层（Conv）**：提取局部特征。

**激活层（ReLU）**：增加非线性能力。

**池化层（Pooling）**：降低特征图尺寸，保留主特征。

**全连接层（FC）**：进行最终的分类或回归。

**Softmax 层**：输出概率分布。

### 3. 应用场景

**图像分类**（如 ResNet、VGG）

**目标检测**（如 YOLO、Faster R-CNN）

**图像分割**（如 U-Net）

**视频分析**（动作识别）

**语音处理**（通过对频谱图卷积）



## 词嵌入

词嵌入的作用是：**把“词”变成“向量”方便神经网络理解。**

### 为什么需要词嵌入？

计算机不能直接理解文字，只能处理数字;

如果用 One-hot (独热）表示词语，会非常稀疏，效果不好;

用向量表示词语，可以让语义相近的词，向量也更接近。

### 词嵌入能做什么？

表达语义，比如“猫”和“狗”向量更接近，“猫”和“汽车”就远；

让神经网络更容易学习语言规律；

作为 NLP 模型的输入，比如输入给 RNN、LSTM、Transformer。

### 常见的词嵌入方法

- **Word2Vec**

- **GloVe**

- **FastText**

- 现代模型常用 **上下文词向量（如 BERT）**

	

------

## RNN

**循环神经网络**(RNN) 是一种专门用来处理**序列数据**（时间序列、文本、语音等）的神经网络。

### RNN 的核心思想

当前的输入不仅影响当前输出，还会影响接下来的输出；

它可以“记住一些之前的信息”；

所以适合处理一句话这种有先后顺序的数据。

例如输入一句话：“我 爱 深度 学习”，RNN 会按顺序一个词一个词地读入，同时把前面的记忆带到后面。

### RNN 的优点

- 能处理顺序信息
- 能理解上下文，比如看到“我爱”，后面更可能出现“你”、“吃”、“学习”等

### RNN 的不足

- 记忆能力有限，长句子效果不好
- 训练时容易出现梯度消失问题

### 常见改进版本

- **LSTM**：增强“记忆能力”
- **GRU**：结构更简单，但效果接近 LSTM



